{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.pyc'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnlm; reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import rnnlm; reload(rnnlm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "  lm.BuildCoreGraph()\n",
    "  lm.BuildTrainGraph()\n",
    "  lm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = lm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = lm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    np.random.seed(168)\n",
    "\n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "        h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    " \n",
    "    cost, h, _ = session.run([loss, lm.final_h_, train_op], feed_dict= {lm.target_y_: y, lm.initial_h_:h,\n",
    "        lm.input_w_: w, lm.dropout_keep_prob_:keep_prob, lm.learning_rate_:learning_rate})      \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grand model starts at: Mon, 19 Dec 2016 19:06:58\n",
      "Mon, 19 Dec 2016 19:06:58\n",
      "Mon, 19 Dec 2016 19:06:58 setting up training for 'pos' started Loaded 25932 sentences (603291 tokens)\n",
      "Training set: 20745 sentences (482251 tokens)\n",
      "Test set: 5187 sentences (121040 tokens)\n",
      "Mon, 19 Dec 2016 19:07:49 setting up training for 'pos' ended Mon, 19 Dec 2016 19:07:49 training for 'pos' startedWARNING:tensorflow:From <ipython-input-29-e9e3b51baadf>:64 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-e9e3b51baadf>:64 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mon, 19 Dec 2016 19:07:53 epoch for 'pos' started [epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:04:22\n",
      "[epoch 1] Train set: avg. loss: 50.362  (perplexity: 7447385840729851953152.00)\n",
      "[epoch 1] Test set: avg. loss: 51.281  (perplexity: 18668923259350855712768.00)\n",
      "\n",
      "Mon, 19 Dec 2016 19:15:16 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:15:16 epoch for 'pos' started [epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:02:25\n",
      "[epoch 2] Train set: avg. loss: 6.365  (perplexity: 581.37)\n",
      "[epoch 2] Test set: avg. loss: 6.430  (perplexity: 620.44)\n",
      "\n",
      "Mon, 19 Dec 2016 19:20:37 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:20:37 epoch for 'pos' started [epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:02:31\n",
      "[epoch 3] Train set: avg. loss: 6.153  (perplexity: 469.98)\n",
      "[epoch 3] Test set: avg. loss: 6.242  (perplexity: 513.86)\n",
      "\n",
      "Mon, 19 Dec 2016 19:26:09 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:26:09 epoch for 'pos' started [epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:02:20\n",
      "[epoch 4] Train set: avg. loss: 5.930  (perplexity: 376.26)\n",
      "[epoch 4] Test set: avg. loss: 6.036  (perplexity: 418.35)\n",
      "\n",
      "Mon, 19 Dec 2016 19:31:27 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:31:27 epoch for 'pos' started [epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:02:19\n",
      "[epoch 5] Train set: avg. loss: 5.752  (perplexity: 314.86)\n",
      "[epoch 5] Test set: avg. loss: 5.879  (perplexity: 357.38)\n",
      "\n",
      "Mon, 19 Dec 2016 19:36:42 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:36:42 epoch for 'pos' started [epoch 6] Starting epoch 6\n",
      "[epoch 6] Completed in 0:02:21\n",
      "[epoch 6] Train set: avg. loss: 5.597  (perplexity: 269.65)\n",
      "[epoch 6] Test set: avg. loss: 5.742  (perplexity: 311.79)\n",
      "\n",
      "Mon, 19 Dec 2016 19:41:50 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:41:50 epoch for 'pos' started [epoch 7] Starting epoch 7\n",
      "[epoch 7] Completed in 0:02:11\n",
      "[epoch 7] Train set: avg. loss: 5.519  (perplexity: 249.29)\n",
      "[epoch 7] Test set: avg. loss: 5.684  (perplexity: 294.22)\n",
      "\n",
      "Mon, 19 Dec 2016 19:46:24 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:46:24 epoch for 'pos' started [epoch 8] Starting epoch 8\n",
      "[epoch 8] Completed in 0:02:20\n",
      "[epoch 8] Train set: avg. loss: 5.413  (perplexity: 224.25)\n",
      "[epoch 8] Test set: avg. loss: 5.598  (perplexity: 269.96)\n",
      "\n",
      "Mon, 19 Dec 2016 19:51:40 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:51:40 epoch for 'pos' started [epoch 9] Starting epoch 9\n",
      "[epoch 9] Completed in 0:02:22\n",
      "[epoch 9] Train set: avg. loss: 5.331  (perplexity: 206.65)\n",
      "[epoch 9] Test set: avg. loss: 5.546  (perplexity: 256.20)\n",
      "\n",
      "Mon, 19 Dec 2016 19:56:55 epoch for 'pos' ended and saved Mon, 19 Dec 2016 19:56:55 epoch for 'pos' started [epoch 10] Starting epoch 10\n",
      "[epoch 10] Completed in 0:02:12\n",
      "[epoch 10] Train set: avg. loss: 5.243  (perplexity: 189.21)\n",
      "[epoch 10] Test set: avg. loss: 5.482  (perplexity: 240.39)\n",
      "\n",
      "Mon, 19 Dec 2016 20:02:11 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:02:11 epoch for 'pos' started [epoch 11] Starting epoch 11\n",
      "[epoch 11] Completed in 0:02:17\n",
      "[epoch 11] Train set: avg. loss: 5.193  (perplexity: 180.05)\n",
      "[epoch 11] Test set: avg. loss: 5.451  (perplexity: 233.03)\n",
      "\n",
      "Mon, 19 Dec 2016 20:07:35 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:07:35 epoch for 'pos' started [epoch 12] Starting epoch 12\n",
      "[epoch 12] Completed in 0:02:19\n",
      "[epoch 12] Train set: avg. loss: 5.133  (perplexity: 169.50)\n",
      "[epoch 12] Test set: avg. loss: 5.417  (perplexity: 225.21)\n",
      "\n",
      "Mon, 19 Dec 2016 20:12:55 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:12:55 epoch for 'pos' started [epoch 13] Starting epoch 13\n",
      "[epoch 13] Completed in 0:02:26\n",
      "[epoch 13] Train set: avg. loss: 5.105  (perplexity: 164.79)\n",
      "[epoch 13] Test set: avg. loss: 5.416  (perplexity: 225.00)\n",
      "\n",
      "Mon, 19 Dec 2016 20:18:24 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:18:24 epoch for 'pos' started [epoch 14] Starting epoch 14\n",
      "[epoch 14] Completed in 0:02:27\n",
      "[epoch 14] Train set: avg. loss: 5.041  (perplexity: 154.65)\n",
      "[epoch 14] Test set: avg. loss: 5.378  (perplexity: 216.49)\n",
      "\n",
      "Mon, 19 Dec 2016 20:23:53 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:23:53 epoch for 'pos' started [epoch 15] Starting epoch 15\n",
      "[epoch 15] Completed in 0:02:15\n",
      "[epoch 15] Train set: avg. loss: 5.014  (perplexity: 150.49)\n",
      "[epoch 15] Test set: avg. loss: 5.374  (perplexity: 215.70)\n",
      "\n",
      "Mon, 19 Dec 2016 20:29:13 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:29:13 epoch for 'pos' started [epoch 16] Starting epoch 16\n",
      "[epoch 16] Completed in 0:02:19\n",
      "[epoch 16] Train set: avg. loss: 4.980  (perplexity: 145.46)\n",
      "[epoch 16] Test set: avg. loss: 5.360  (perplexity: 212.69)\n",
      "\n",
      "Mon, 19 Dec 2016 20:34:38 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:34:38 epoch for 'pos' started [epoch 17] Starting epoch 17\n",
      "[epoch 17] Completed in 0:02:21\n",
      "[epoch 17] Train set: avg. loss: 4.954  (perplexity: 141.75)\n",
      "[epoch 17] Test set: avg. loss: 5.360  (perplexity: 212.73)\n",
      "\n",
      "Mon, 19 Dec 2016 20:40:01 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:40:01 epoch for 'pos' started [epoch 18] Starting epoch 18\n",
      "[epoch 18] Completed in 0:02:22\n",
      "[epoch 18] Train set: avg. loss: 4.929  (perplexity: 138.25)\n",
      "[epoch 18] Test set: avg. loss: 5.352  (perplexity: 211.11)\n",
      "\n",
      "Mon, 19 Dec 2016 20:45:22 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:45:22 epoch for 'pos' started [epoch 19] Starting epoch 19\n",
      "[epoch 19] Completed in 0:02:24\n",
      "[epoch 19] Train set: avg. loss: 4.896  (perplexity: 133.70)\n",
      "[epoch 19] Test set: avg. loss: 5.343  (perplexity: 209.16)\n",
      "\n",
      "Mon, 19 Dec 2016 20:50:49 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:50:49 epoch for 'pos' started [epoch 20] Starting epoch 20\n",
      "[epoch 20] Completed in 0:02:17\n",
      "[epoch 20] Train set: avg. loss: 4.872  (perplexity: 130.62)\n",
      "[epoch 20] Test set: avg. loss: 5.343  (perplexity: 209.20)\n",
      "\n",
      "Mon, 19 Dec 2016 20:56:05 epoch for 'pos' ended and saved Mon, 19 Dec 2016 20:56:08 training for 'pos' ended Mon, 19 Dec 2016 20:56:08 setting up training for 'neg' started Loaded 25943 sentences (547369 tokens)\n",
      "Training set: 20754 sentences (436986 tokens)\n",
      "Test set: 5189 sentences (110383 tokens)\n",
      "Mon, 19 Dec 2016 20:57:42 setting up training for 'neg' ended Mon, 19 Dec 2016 20:57:42 training for 'neg' startedWARNING:tensorflow:From <ipython-input-29-e9e3b51baadf>:64 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-e9e3b51baadf>:64 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mon, 19 Dec 2016 20:57:49 epoch for 'neg' started [epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:04:38\n",
      "[epoch 1] Train set: avg. loss: 48.241  (perplexity: 892809828113443323904.00)\n",
      "[epoch 1] Test set: avg. loss: 49.057  (perplexity: 2019345912302082457600.00)\n",
      "\n",
      "Mon, 19 Dec 2016 21:04:59 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:04:59 epoch for 'neg' started [epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:02:31\n",
      "[epoch 2] Train set: avg. loss: 7.943  (perplexity: 2816.40)\n",
      "[epoch 2] Test set: avg. loss: 8.039  (perplexity: 3099.42)\n",
      "\n",
      "Mon, 19 Dec 2016 21:09:05 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:09:05 epoch for 'neg' started [epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:02:29\n",
      "[epoch 3] Train set: avg. loss: 7.796  (perplexity: 2430.38)\n",
      "[epoch 3] Test set: avg. loss: 7.911  (perplexity: 2728.04)\n",
      "\n",
      "Mon, 19 Dec 2016 21:13:09 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:13:09 epoch for 'neg' started [epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:02:26\n",
      "[epoch 4] Train set: avg. loss: 7.644  (perplexity: 2088.01)\n",
      "[epoch 4] Test set: avg. loss: 7.772  (perplexity: 2372.10)\n",
      "\n",
      "Mon, 19 Dec 2016 21:17:19 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:17:19 epoch for 'neg' started [epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:02:29\n",
      "[epoch 5] Train set: avg. loss: 7.733  (perplexity: 2283.43)\n",
      "[epoch 5] Test set: avg. loss: 7.865  (perplexity: 2603.88)\n",
      "\n",
      "Mon, 19 Dec 2016 21:21:22 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:21:22 epoch for 'neg' started [epoch 6] Starting epoch 6\n",
      "[epoch 6] Completed in 0:02:27\n",
      "[epoch 6] Train set: avg. loss: 7.581  (perplexity: 1960.67)\n",
      "[epoch 6] Test set: avg. loss: 7.696  (perplexity: 2200.56)\n",
      "\n",
      "Mon, 19 Dec 2016 21:25:24 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:25:24 epoch for 'neg' started [epoch 7] Starting epoch 7\n",
      "[epoch 7] Completed in 0:02:24\n",
      "[epoch 7] Train set: avg. loss: 7.517  (perplexity: 1838.94)\n",
      "[epoch 7] Test set: avg. loss: 7.624  (perplexity: 2045.92)\n",
      "\n",
      "Mon, 19 Dec 2016 21:29:38 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:29:38 epoch for 'neg' started [epoch 8] Starting epoch 8\n",
      "[epoch 8] Completed in 0:02:42\n",
      "[epoch 8] Train set: avg. loss: 7.448  (perplexity: 1716.41)\n",
      "[epoch 8] Test set: avg. loss: 7.540  (perplexity: 1881.54)\n",
      "\n",
      "Mon, 19 Dec 2016 21:34:17 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:34:17 epoch for 'neg' started [epoch 9] Starting epoch 9\n",
      "[epoch 9] Completed in 0:02:39\n",
      "[epoch 9] Train set: avg. loss: 7.317  (perplexity: 1505.22)\n",
      "[epoch 9] Test set: avg. loss: 7.400  (perplexity: 1636.73)\n",
      "\n",
      "Mon, 19 Dec 2016 21:38:54 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:38:54 epoch for 'neg' started [epoch 10] Starting epoch 10\n",
      "[epoch 10] Completed in 0:02:37\n",
      "[epoch 10] Train set: avg. loss: 7.295  (perplexity: 1473.13)\n",
      "[epoch 10] Test set: avg. loss: 7.370  (perplexity: 1587.03)\n",
      "\n",
      "Mon, 19 Dec 2016 21:43:28 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:43:28 epoch for 'neg' started [epoch 11] Starting epoch 11\n",
      "[epoch 11] Completed in 0:02:42\n",
      "[epoch 11] Train set: avg. loss: 7.250  (perplexity: 1407.77)\n",
      "[epoch 11] Test set: avg. loss: 7.321  (perplexity: 1511.44)\n",
      "\n",
      "Mon, 19 Dec 2016 21:48:07 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:48:07 epoch for 'neg' started [epoch 12] Starting epoch 12\n",
      "[epoch 12] Completed in 0:02:31\n",
      "[epoch 12] Train set: avg. loss: 7.165  (perplexity: 1292.88)\n",
      "[epoch 12] Test set: avg. loss: 7.231  (perplexity: 1381.41)\n",
      "\n",
      "Mon, 19 Dec 2016 21:52:35 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:52:35 epoch for 'neg' started [epoch 13] Starting epoch 13\n",
      "[epoch 13] Completed in 0:02:34\n",
      "[epoch 13] Train set: avg. loss: 7.102  (perplexity: 1214.28)\n",
      "[epoch 13] Test set: avg. loss: 7.162  (perplexity: 1289.84)\n",
      "\n",
      "Mon, 19 Dec 2016 21:57:05 epoch for 'neg' ended and saved Mon, 19 Dec 2016 21:57:05 epoch for 'neg' started [epoch 14] Starting epoch 14\n",
      "[epoch 14] Completed in 0:02:32\n",
      "[epoch 14] Train set: avg. loss: 7.095  (perplexity: 1206.10)\n",
      "[epoch 14] Test set: avg. loss: 7.159  (perplexity: 1285.41)\n",
      "\n",
      "Mon, 19 Dec 2016 22:01:32 epoch for 'neg' ended and saved Mon, 19 Dec 2016 22:01:32 epoch for 'neg' started [epoch 15] Starting epoch 15\n",
      "[epoch 15] Completed in 0:02:20\n",
      "[epoch 15] Train set: avg. loss: 6.891  (perplexity: 982.96)\n",
      "[epoch 15] Test set: avg. loss: 6.956  (perplexity: 1049.32)\n",
      "\n",
      "Mon, 19 Dec 2016 22:05:49 epoch for 'neg' ended and saved Mon, 19 Dec 2016 22:05:49 epoch for 'neg' started [epoch 16] Starting epoch 16\n",
      "[epoch 16] Completed in 0:02:06\n",
      "[epoch 16] Train set: avg. loss: 6.612  (perplexity: 743.65)\n",
      "[epoch 16] Test set: avg. loss: 6.688  (perplexity: 802.57)\n",
      "\n",
      "Mon, 19 Dec 2016 22:09:50 epoch for 'neg' ended and saved Mon, 19 Dec 2016 22:09:50 epoch for 'neg' started [epoch 17] Starting epoch 17\n",
      "[epoch 17] Completed in 0:01:46\n",
      "[epoch 17] Train set: avg. loss: 6.319  (perplexity: 555.26)\n",
      "[epoch 17] Test set: avg. loss: 6.425  (perplexity: 616.81)\n",
      "\n",
      "Mon, 19 Dec 2016 22:13:31 epoch for 'neg' ended and saved Mon, 19 Dec 2016 22:13:31 epoch for 'neg' started [epoch 18] Starting epoch 18\n",
      "[epoch 18] Completed in 0:01:44\n",
      "[epoch 18] Train set: avg. loss: 6.163  (perplexity: 474.75)\n",
      "[epoch 18] Test set: avg. loss: 6.273  (perplexity: 529.99)\n",
      "\n",
      "Mon, 19 Dec 2016 22:17:09 epoch for 'neg' ended and saved Mon, 19 Dec 2016 22:17:09 epoch for 'neg' started [epoch 19] Starting epoch 19\n",
      "[epoch 19] Completed in 0:01:46\n",
      "[epoch 19] Train set: avg. loss: 5.998  (perplexity: 402.77)\n",
      "[epoch 19] Test set: avg. loss: 6.128  (perplexity: 458.48)\n",
      "\n",
      "Mon, 19 Dec 2016 22:20:51 epoch for 'neg' ended and saved Mon, 19 Dec 2016 22:20:51 epoch for 'neg' started [epoch 20] Starting epoch 20\n",
      "[epoch 20] Completed in 0:01:48\n",
      "[epoch 20] Train set: avg. loss: 5.820  (perplexity: 337.10)\n",
      "[epoch 20] Test set: avg. loss: 5.967  (perplexity: 390.45)\n",
      "\n",
      "Mon, 19 Dec 2016 22:24:34 epoch for 'neg' ended and saved Mon, 19 Dec 2016 22:24:36 training for 'neg' ended\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import time\n",
    "reload(utils)\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "\n",
    "print \"Grand model starts at:\", time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "\n",
    "np.random.seed(168)\n",
    "\n",
    "lm_name = 'mr' # which is also the corpus name which is also the folder name in local directory in which each category\n",
    "               # is specified as a txt file with <category_name>.txt as the filename.\n",
    "\n",
    "corpus = nltk.corpus.CategorizedPlaintextCorpusReader('./'+lm_name+'/', r'.*\\.txt', cat_pattern=r'(\\w+)\\.txt')\n",
    "\n",
    "categories_train_set = ['pos', 'neg']\n",
    "\n",
    "V = 10000\n",
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 20\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100, \n",
    "                    num_layers=3)\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(lm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))\n",
    "\n",
    "for categories in categories_train_set:\n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()), \"setting up training for '\"+categories+\"' started\", \n",
    "    \n",
    "    vocab, train_ids, test_ids = utils.load_corpus(corpus, split=0.8, V=V, categories=categories, shuffle=True)\n",
    "\n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()), \"setting up training for '\"+categories+\"' ended\", \n",
    "\n",
    "\n",
    "    trained_filename = './tf_saved/rnnlm_trained' + '_' + lm_name + '_' + categories\n",
    "    \n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()), \"training for '\"+categories+\"' started\", \n",
    "\n",
    "    with tf.Graph().as_default(), tf.Session() as session:\n",
    "      # Seed RNG for repeatability\n",
    "      tf.set_random_seed(168)\n",
    "\n",
    "      with tf.variable_scope(\"model\", reuse=None):\n",
    "        lm = rnnlm.RNNLM(**model_params)\n",
    "        lm.BuildCoreGraph()\n",
    "        lm.BuildTrainGraph()\n",
    "\n",
    "      session.run(tf.initialize_all_variables())\n",
    "      saver = tf.train.Saver()\n",
    "\n",
    "      for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "        print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()), \"epoch for '\"+categories+\"' started\", \n",
    "\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        #### YOUR CODE HERE ####\n",
    "\n",
    "        run_epoch(lm, session, bi, train=True, keep_prob=keep_prob, learning_rate=learning_rate)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "\n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print \"\"\n",
    "\n",
    "        # Save a checkpoint\n",
    "        saver.save(session, './tf_saved/rnnlm' + '_' + lm_name + '_' + categories, global_step=epoch)\n",
    "        print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()), \"epoch for '\"+categories+\"' ended and saved\", \n",
    "      # Save final model\n",
    "      saver.save(session, trained_filename)\n",
    "        \n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()), \"training for '\"+categories+\"' ended\", \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    lm : rnnlm.RNNLM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  final_h, samples = session.run([lm.final_h_, lm.pred_samples_], \n",
    "        feed_dict={lm.input_w_: input_w, lm.initial_h_: initial_h, lm.dropout_keep_prob_: 1.0, lm.learning_rate_:0.1})\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> improve only him high scene , being the monster coach standout in j in a <unk> , that painfully sorry \n",
      "<s> it ' t painfully playing i painfully commonplace in , there are as the millenium ( <unk> possibilities few of \n",
      "<s> hence DG ? <s> \n",
      "<s> gratuitous juan child style , \" is \" some , was juan . <s> \n",
      "<s> * gratuitous \" food to . <s> \n",
      "<s> is even gratuitous much the heck is we behind <unk> , noteworthy , no interesting that , the cannes that \n",
      "<s> gratuitous end members back . <s> \n",
      "<s> a bruce incoherent julianne times action and possibly the columbia de big on a 70s in a no middle among \n",
      "<s> carrey ' t \" julianne . <s> \n",
      "<s> expects knocking he ' s eagerly , painfully selected . <s> \n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "reload(rnnlm)\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 168\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(random_seed)\n",
    "\n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    lm.BuildSamplerGraph()\n",
    "\n",
    "  # Load the trained model\n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(session, trained_filename)\n",
    "\n",
    "  # Make initial state for a batch with batch_size = num_samples\n",
    "  w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "  # We'll take one step for each sequence on each iteration \n",
    "  for i in xrange(max_steps):\n",
    "    h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "    w = np.hstack((w,y))\n",
    "\n",
    "  # Print generated sentences\n",
    "  for row in w:\n",
    "    for i, word_id in enumerate(row):\n",
    "      print vocab.id_to_word[word_id],\n",
    "      if (i != 0) and (word_id == vocab.START_ID):\n",
    "        break\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "  \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "  feed_dict = {lm.input_w_:w,\n",
    "               lm.target_y_:y,\n",
    "               lm.initial_h_:h,\n",
    "               lm.dropout_keep_prob_: 1.0}\n",
    "  # Return log(P(seq)) = -1*loss\n",
    "  return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False, silent=False):\n",
    "  \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "  with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "      tf.set_random_seed(168)\n",
    "      lm = rnnlm.RNNLM(**model_params)\n",
    "      lm.BuildCoreGraph()\n",
    " \n",
    "    # print 'loading', trained_filename\n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, trained_filename)\n",
    "  \n",
    "    if isinstance(inputs[0], str) or isinstance(inputs[0], unicode):\n",
    "      inputs = [inputs]\n",
    "\n",
    "    # Actually run scoring\n",
    "    results = []\n",
    "    for words in inputs:\n",
    "      score = score_seq(lm, session, words, vocab)\n",
    "      results.append((score, words))\n",
    "    \n",
    "    # Sort if requested\n",
    "    if sort: results = sorted(results, reverse=True)\n",
    "    \n",
    "    # Print results\n",
    "    for score, words in results:\n",
    "      if not silent:\n",
    "          print \"\\\"%s\\\" : %.05f\" % (\" \".join(words), score)\n",
    "    if silent:\n",
    "      return [score for score, words in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon, 19 Dec 2016 22:24:38 scoring started tp = 3531, fp = 359, tn = 180, fn = 1154, nopt = 0, nont = 0\n",
      "\n",
      "precision = 90.77, recall = 75.37, accuracy = 71.04, true_neg_rate = 33.40, f-measure = 82.36\n",
      "\n",
      "\n",
      "Mon, 19 Dec 2016 22:33:04 scoring ended\n"
     ]
    }
   ],
   "source": [
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()), \"scoring started\", \n",
    "\n",
    "#neg predictor\n",
    "total_pos = 0\n",
    "total_neg = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "fp = 0\n",
    "#num_sample = 10000\n",
    "nopt = 0\n",
    "nont = 0\n",
    "\n",
    "categories = 'pos'\n",
    "trained_filename = './tf_saved/rnnlm_trained' + '_' + lm_name + '_' +categories\n",
    "\n",
    "pos_for_pos_score = load_and_score(corpus.sents(categories = 'posdev'), silent=True)\n",
    "neg_for_pos_score = load_and_score(corpus.sents(categories = 'negdev'), silent=True)\n",
    "\n",
    "\n",
    "categories = 'neg'\n",
    "trained_filename = './tf_saved/rnnlm_trained' + '_' + lm_name + '_' + categories\n",
    "\n",
    "pos_for_neg_score = load_and_score(corpus.sents(categories = 'posdev'), silent=True)\n",
    "neg_for_neg_score = load_and_score(corpus.sents(categories = 'negdev'), silent=True)\n",
    "\n",
    "for i in range(len(dev_sents['pos'])):\n",
    "    \n",
    "    if pos_for_pos_score[i] > pos_for_neg_score[i]:\n",
    "        tp = tp + 1\n",
    "    else:\n",
    "        if pos_for_pos_score[i] < pos_for_neg_score[i]:\n",
    "            fp = fp + 1\n",
    "        else: #flip a coin\n",
    "            nopt = nopt + 1\n",
    "            if np.random.randint(0, 1) == 0:\n",
    "                tp = tp + 1\n",
    "            else: \n",
    "                fp = fp + 1\n",
    "\n",
    "for i in range(len(dev_sents['neg'])):\n",
    "\n",
    "    if neg_for_pos_score[i] < neg_for_neg_score[i]:\n",
    "        tn = tn + 1\n",
    "    else:\n",
    "        if neg_for_pos_score[i] > neg_for_neg_score[i]:\n",
    "            fn = fn + 1\n",
    "        else: #flip a coin\n",
    "            nont = nont + 1\n",
    "            if np.random.randint(0, 1) == 0:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "\n",
    "print \"tp = %d, fp = %d, tn = %d, fn = %d, nopt = %d, nont = %d\\n\" % (tp, fp, tn, fn, nopt, nont)\n",
    "\n",
    "precision = tp * 1.0 / (tp+fp)\n",
    "recall = tp * 1.0 / (tp+fn)\n",
    "accuracy = (tp + tn) * 1.0 / (tp+fp+tn+fn)\n",
    "true_neg_rate = tn * 1.0 / (tn + fp)\n",
    "f_measure = 2.0 * precision * recall / (precision + recall)\n",
    "\n",
    "print \"precision = %.2f, recall = %.2f, accuracy = %.2f, true_neg_rate = %.2f, f-measure = %.2f\\n\" % \\\n",
    "      (precision*100, recall*100, accuracy*100, true_neg_rate*100, f_measure*100)\n",
    "print\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime()), \"scoring ended\", \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
